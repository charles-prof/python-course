{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQRQmBIssKomJ92MG/sKe4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charles-prof/python-course/blob/main/notebooks/tlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oU8dRQs2E6Dj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acf1da70"
      },
      "source": [
        "# Task\n",
        "Develop a text-to-silhouette model that generates 128x128 clean silhouettes from word-meaning pairs, using a Text Encoder (DistilBERT) and a Generative Backend (VQ-VAE and Transformer). The total model parameters must be under 250 million, and the final model should be optimized for CPU inference to achieve sub-second generation times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7a0939"
      },
      "source": [
        "## Implement Text Encoder\n",
        "\n",
        "### Subtask:\n",
        "Implement the Text Encoder using a pre-trained DistilBERT model. This will process the input word and its meaning to generate semantic embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7027ee5f"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary classes, `DistilBertModel` and `DistilBertTokenizer`, from the `transformers` library, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e0c6f93",
        "outputId": "1011269f-1114-43b1-e33d-fcf5bcbb9e94"
      },
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "print(\"DistilBertModel and DistilBertTokenizer imported successfully.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertModel and DistilBertTokenizer imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9968ce01"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully imported the required classes. Now, I will load the pre-trained DistilBERT tokenizer and model, and then define a function to process word-meaning pairs into semantic embeddings, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b855d16",
        "outputId": "fb8f66b0-3770-430f-a8b4-160aefdb27dc"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def get_text_embedding(word, meaning):\n",
        "    text = f\"{word}: {meaning}\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    # Get the last hidden state of the [CLS] token (first token)\n",
        "    # This is often used as a sentence embedding\n",
        "    semantic_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return semantic_embedding\n",
        "\n",
        "print(\"DistilBertTokenizer and DistilBertModel loaded, and get_text_embedding function defined.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertTokenizer and DistilBertModel loaded, and get_text_embedding function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62f37fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully loaded the DistilBERT model and tokenizer and defined the `get_text_embedding` function. The next logical step is to test this function with example word-meaning pairs to ensure it correctly generates semantic embeddings and to verify the output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c12a516",
        "outputId": "c2d63bef-699c-4d63-dbb9-9697eeaf7c28"
      },
      "source": [
        "word1 = \"cat\"\n",
        "meaning1 = \"a small domesticated carnivorous mammal with soft fur, a short snout, and retractile claws.\"\n",
        "embedding1 = get_text_embedding(word1, meaning1)\n",
        "\n",
        "word2 = \"dog\"\n",
        "meaning2 = \"a domesticated carnivorous mammal that typically has a long snout, an acute sense of smell, non-retractable claws, and a barking, howling, or whining voice.\"\n",
        "embedding2 = get_text_embedding(word2, meaning2)\n",
        "\n",
        "print(f\"Embedding for '{word1}':\")\n",
        "print(embedding1.shape)\n",
        "print(embedding1)\n",
        "\n",
        "print(f\"\\nEmbedding for '{word2}':\")\n",
        "print(embedding2.shape)\n",
        "print(embedding2)\n",
        "print(\"Text Encoder tested successfully with example inputs.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'cat':\n",
            "torch.Size([1, 768])\n",
            "tensor([[-8.1034e-01, -2.2354e-01, -5.6860e-01, -1.1459e-01, -8.2875e-02,\n",
            "          2.3735e-01,  3.3038e-01,  4.9816e-01, -2.8395e-01, -1.7168e-01,\n",
            "         -2.1998e-01, -3.0345e-01, -3.8113e-01,  1.2539e-01, -2.3802e-01,\n",
            "          1.9248e-01, -2.0815e-02,  4.9048e-01, -1.2512e-01,  2.2978e-01,\n",
            "          4.3836e-03, -6.9055e-01,  7.8853e-02,  4.5119e-02,  3.8186e-01,\n",
            "          1.0648e-01,  2.5167e-01,  5.9540e-01,  2.3965e-01,  1.3111e-01,\n",
            "          1.6572e-02, -7.3315e-02, -3.3718e-02, -4.0299e-01, -1.2978e-02,\n",
            "         -5.9870e-02,  1.4866e-01, -1.6274e-01,  1.9533e-01,  2.0623e-01,\n",
            "          2.6817e-01,  4.2780e-01, -2.7066e-02,  3.8251e-01, -9.8338e-02,\n",
            "         -4.5478e-01, -2.9955e+00, -4.0255e-02, -3.3602e-01, -1.4257e-01,\n",
            "          1.8147e-01,  1.1181e-01,  1.6918e-01,  6.7212e-01,  3.2586e-01,\n",
            "          2.0396e-01, -5.6714e-01,  4.7465e-02,  4.8946e-02, -1.6108e-02,\n",
            "         -1.2764e-01,  3.9408e-01, -1.6346e-01,  1.6662e-02, -2.8420e-01,\n",
            "          2.8540e-01, -1.0265e-01,  2.0012e-01, -4.2025e-01,  7.5488e-01,\n",
            "         -3.4392e-01, -1.9793e-01,  2.8503e-02,  2.3085e-01, -1.1553e-01,\n",
            "          3.0989e-02, -9.1090e-02,  1.6409e-01, -6.1488e-02,  1.1972e-01,\n",
            "          2.5884e-01,  6.6864e-01,  5.9674e-01,  4.6447e-02, -2.4143e-01,\n",
            "          2.6462e-01, -3.0801e-01, -2.8454e-01, -3.3380e-01,  4.9605e-01,\n",
            "         -3.4634e-01, -1.2561e-01, -6.1700e-02,  3.1577e-01,  3.8576e-01,\n",
            "         -3.2558e-01, -4.6031e-02, -7.1816e-02,  3.3819e-01,  1.0544e-02,\n",
            "          3.5186e-01, -2.2290e-01,  2.5697e-01, -2.1291e-01, -8.4882e-02,\n",
            "         -1.7838e-01, -2.7422e-01, -5.4004e-01, -3.1732e-01, -1.6685e+00,\n",
            "         -2.2454e-01,  1.6399e-01, -9.7964e-03, -6.0353e-01, -2.6983e-01,\n",
            "          6.7402e-01,  1.4592e-01,  2.0209e-02, -2.8118e-01, -2.1137e-01,\n",
            "          1.6398e-01, -5.5478e-03, -1.5883e-02,  3.6244e-02,  2.1690e-03,\n",
            "          3.6887e-01, -8.6896e-02, -2.4087e-01,  4.5246e-01,  1.7904e-01,\n",
            "          6.7128e-01,  5.4512e-01,  8.0591e-02, -1.5797e-01, -4.0565e-01,\n",
            "          3.8166e-01,  1.2823e-01, -1.2979e-01, -3.5334e-01,  1.5016e-01,\n",
            "         -5.9794e-01, -1.0515e-01, -2.5195e+00,  1.9810e-01,  6.5641e-01,\n",
            "          4.4966e-01,  4.2147e-01, -3.6792e-01, -3.3394e-01, -2.1179e-01,\n",
            "          1.4889e-01,  2.5382e-01, -1.1705e-01, -5.7919e-02, -3.5206e-01,\n",
            "         -3.4104e-01,  2.3730e-02, -2.4761e-01,  1.7995e-01,  2.2189e-01,\n",
            "          3.0298e-01, -1.5649e-01, -5.1798e-01, -4.2284e-01, -2.0391e-01,\n",
            "          2.5780e-01,  3.6155e-01,  7.8454e-02, -2.9277e-01,  2.7516e-01,\n",
            "          1.0956e-01,  8.4057e-02,  1.0022e+00, -1.3942e-01,  3.7683e-01,\n",
            "         -1.2998e-01, -1.2640e-01,  3.0001e-01,  5.1572e-01, -8.1264e-02,\n",
            "         -2.5717e-01,  4.8434e-01,  2.1248e-01,  1.2838e-01,  2.8289e-03,\n",
            "         -3.6540e-01,  5.3220e-02, -4.4575e-01, -3.3534e-01, -3.6716e-02,\n",
            "          1.8865e-02, -8.3118e-02, -2.9207e-01,  4.9393e-01,  4.0965e-01,\n",
            "         -8.7543e-02, -2.6092e-01, -8.5830e-01,  6.9515e-02,  2.2814e-02,\n",
            "         -1.6147e-01, -2.2932e-01, -8.8883e-03,  2.2089e-01, -4.8364e-01,\n",
            "          3.3372e+00,  6.1809e-02, -2.4289e-01,  5.6634e-01,  4.9117e-01,\n",
            "         -5.4438e-02, -1.6806e-01, -4.7939e-01, -4.8309e-02,  3.5702e-01,\n",
            "          4.3868e-02,  1.8341e-03,  1.5380e-01, -6.0079e-02, -1.5758e-01,\n",
            "          1.0347e-01,  2.3131e-01, -3.8734e-01,  2.6459e-01, -1.8353e-01,\n",
            "          3.0038e-01,  6.6051e-01,  1.9554e-01,  2.5219e-01, -1.1999e+00,\n",
            "          2.6398e-01,  5.9897e-02, -4.4914e-02,  1.4900e-01, -5.4986e-01,\n",
            "         -2.2649e-01, -2.0905e-01, -1.0732e-01,  4.1489e-01, -8.9896e-02,\n",
            "         -3.2985e-01,  5.0338e-02,  5.9711e-01,  2.4925e-02, -2.6989e-02,\n",
            "          8.5833e-03,  2.5077e-01,  5.0584e-01, -7.7398e-02,  5.5196e-02,\n",
            "          1.4414e-01,  5.2018e-01,  5.3571e-01, -4.6186e-02,  4.2947e-01,\n",
            "         -9.0662e-02,  6.7787e-02,  2.0244e-01, -3.3767e-01, -1.9388e-01,\n",
            "         -1.4806e-01,  1.7644e-01,  6.7032e-02, -3.7401e-02, -5.8274e-01,\n",
            "         -8.4381e-02,  3.1191e-01,  3.1348e-02,  9.5207e-02,  6.1061e-02,\n",
            "          1.4238e-01, -3.8587e-02, -7.5229e-01, -2.8540e+00,  2.5670e-02,\n",
            "         -7.6706e-02,  2.1562e-01, -7.5357e-02,  2.1732e-01, -2.6533e-01,\n",
            "          2.7837e-01,  3.4205e-01, -3.6053e-01,  4.4523e-01, -8.8202e-02,\n",
            "         -2.1091e-01,  4.8171e-01, -8.9040e-01,  4.7973e-01, -3.7355e-01,\n",
            "         -5.8290e-01,  6.9962e-02, -2.8699e-01,  2.3318e-01, -4.7442e-01,\n",
            "          8.8924e-02, -2.6986e-02,  2.0112e-01,  4.5713e-01, -8.3571e-01,\n",
            "         -5.8939e-01, -2.4343e-01,  2.4642e-01,  2.3591e-01,  9.8807e-03,\n",
            "         -5.9337e-03, -2.4009e-01, -4.8138e-01, -2.9712e+00,  4.8559e-01,\n",
            "          7.2856e-02, -3.3627e-01,  5.0677e-01, -9.4629e-02,  2.5521e-01,\n",
            "         -2.6933e-01, -4.8115e-01,  2.0083e-01,  3.8574e-01, -8.5180e-02,\n",
            "          1.1709e-01, -7.0196e-02,  2.9187e-01, -4.6155e-01,  1.1379e-01,\n",
            "         -3.5796e-01,  9.3218e-02, -1.7536e-01,  2.5696e-01, -1.3041e-01,\n",
            "         -5.3152e-01, -1.7186e-01,  4.2535e-01,  2.7336e-01, -1.9785e-01,\n",
            "         -4.6815e-01, -2.6187e-01, -4.5633e-01, -3.6102e-02, -4.4161e-01,\n",
            "         -1.4527e-01,  1.1205e-01,  5.8403e-02, -1.5807e-01, -2.4393e-02,\n",
            "          1.5748e-01,  3.3419e-01,  9.0875e-02, -1.1873e-01,  8.5475e-01,\n",
            "          4.5259e-01,  2.3616e-01,  9.7358e-01,  2.3865e-01,  5.2896e-01,\n",
            "         -5.2141e-01,  4.0777e-02,  3.1874e-01, -1.5635e-01,  2.4539e-02,\n",
            "          7.8065e-01,  3.9384e-01,  4.7273e-01, -4.6569e-01,  4.4111e-01,\n",
            "         -1.9913e-02, -4.9171e-01,  1.2747e-01,  6.7279e-01, -1.1929e-01,\n",
            "         -5.6321e-02, -2.2424e-02,  2.9884e-01, -1.9712e-01,  3.6187e-02,\n",
            "         -6.3864e-01,  1.3583e-01, -1.0898e-02, -5.1435e-02,  4.3960e-01,\n",
            "         -2.3546e-01, -1.0992e+00, -1.4032e-01, -4.0718e-01,  1.0792e-01,\n",
            "         -1.1056e-01, -1.8900e-01, -3.2484e-02, -3.9339e-01, -4.5062e-01,\n",
            "         -8.1596e-02,  2.3793e-01, -4.2748e-01, -5.8117e-01, -4.0419e-01,\n",
            "          2.4254e-01, -5.2526e-01, -4.5337e-01, -2.2167e-01, -9.6293e-02,\n",
            "          1.8822e-01,  8.0169e-02, -3.0008e-04, -2.3984e-01,  5.4024e-01,\n",
            "         -8.2263e-01,  6.2349e-01, -2.9147e-01,  4.0244e-02, -3.6269e-01,\n",
            "          6.5497e-02,  2.8624e-01, -9.0531e-02, -6.5860e-01, -3.3614e-01,\n",
            "          3.4651e-01,  1.7996e-01,  3.1857e-02, -2.3966e-01, -2.8188e-01,\n",
            "          1.0649e-01,  1.7849e-01,  1.2729e+00,  3.8963e-01,  3.1070e-01,\n",
            "          2.8091e-01,  3.4150e-01,  3.3336e-01,  2.4963e-01,  8.4921e-02,\n",
            "         -2.2249e-01, -3.5906e-01, -1.9572e-01, -7.0744e-02,  1.0321e-01,\n",
            "         -2.9105e-01, -2.9232e-01, -4.3534e-01,  2.1873e-01,  1.7334e-02,\n",
            "         -3.3878e-01, -4.1373e-01,  2.1840e-01, -9.5090e-02, -5.2412e-01,\n",
            "          9.4076e-02,  5.4299e-01,  4.1994e-02, -2.8760e-02,  1.9225e-01,\n",
            "          2.0462e-03, -1.3483e-01, -1.2193e-02,  2.0990e-01,  2.4615e-01,\n",
            "          1.0114e-01, -1.8346e-02,  5.8140e-01,  2.5925e-01, -1.9296e-01,\n",
            "          1.1957e-01, -4.3328e-01,  1.7665e-01, -2.5338e-01,  3.3271e-01,\n",
            "          2.8189e-01,  1.0807e-02, -9.4093e-02,  8.7854e-03,  3.3313e-01,\n",
            "         -1.5879e+00,  4.9408e-01,  2.1708e-01, -7.1837e-03,  1.6947e-02,\n",
            "          5.9838e-03,  4.8281e-02,  5.4922e-01,  2.0944e-01,  2.9430e-01,\n",
            "         -8.7626e-02, -1.5707e-01, -2.0983e-01, -1.5592e-01, -2.5399e-02,\n",
            "          1.8049e-01, -4.1217e-01, -7.8974e-02,  1.7249e-01, -1.9099e-01,\n",
            "         -2.3965e-01,  2.2959e-02,  5.8264e-01, -2.6091e-02,  2.4310e-01,\n",
            "          3.8731e-03, -4.9005e-02,  3.4257e-01,  1.9033e-01,  2.8265e-01,\n",
            "         -1.7110e-01, -3.7074e-01, -9.3009e-01,  4.7410e-01,  7.7952e-02,\n",
            "          2.1198e-01,  3.0750e-01, -1.6498e-01,  2.6816e-01,  2.5959e-01,\n",
            "         -3.5381e-01,  7.8043e-02,  1.4247e-01,  3.5966e-02,  7.8993e-01,\n",
            "         -1.6174e-01, -4.2925e-01,  2.5277e-02,  3.3912e-02, -6.2826e-02,\n",
            "          1.1393e-01, -1.1824e-01,  2.2131e-01,  9.5997e-02, -1.3854e-01,\n",
            "         -5.5539e-02, -7.4664e-02,  3.3338e-01, -2.7904e-01, -1.7772e-01,\n",
            "          3.9375e-01, -4.2981e-01, -1.1325e-02,  1.7647e-01,  5.1700e-02,\n",
            "         -2.4317e-01,  3.3001e-02, -1.3343e-01,  1.4023e-01, -6.5420e-02,\n",
            "          2.8442e-01, -3.2455e-01, -3.0724e-01,  3.7474e-01, -8.5397e-02,\n",
            "         -2.0037e-01,  1.8387e-01,  3.5682e-02,  5.7205e-01,  2.5998e-01,\n",
            "          3.0466e-01,  1.2493e-01,  1.6821e-01,  4.2931e-01, -2.6710e-01,\n",
            "          9.4891e-02, -2.5134e-01, -3.6492e-01,  2.6982e-01,  3.2817e-01,\n",
            "         -4.4543e-01,  3.2279e-01, -8.1007e-02, -1.4569e-02, -7.7553e-02,\n",
            "          2.2183e-02, -1.6670e-03,  2.6579e-01,  1.5022e-01,  1.1244e-01,\n",
            "         -4.9428e-01,  7.5514e-01,  9.4160e-02, -1.0216e-01, -3.4944e-01,\n",
            "          1.9746e-01,  1.2601e-01,  5.7403e-02, -1.7261e-01, -1.1358e-01,\n",
            "          2.2795e-01, -3.3829e-01, -2.9515e-01,  2.1560e-01,  1.9671e-02,\n",
            "         -5.0212e-01, -4.1754e-01, -1.2596e-01,  1.4938e+00,  4.8797e-01,\n",
            "          4.1818e-01,  2.7674e-01,  2.8824e-01, -5.6653e-02, -1.8275e-01,\n",
            "          1.0001e-01, -1.5260e-01,  2.0518e-01,  4.0588e-03, -1.4182e-01,\n",
            "          1.4650e-01,  1.9928e-01,  4.9340e-01,  2.5062e-01, -2.5875e-02,\n",
            "         -3.3462e-01, -3.0729e-01,  4.4722e-01, -3.4278e-01,  7.6232e-01,\n",
            "          4.9244e-01, -1.3173e-01,  2.6577e-01, -7.9847e-02, -1.0019e-01,\n",
            "         -2.3703e-02,  5.7144e-02,  2.7488e-01, -1.2797e-01,  1.6960e-01,\n",
            "          1.1787e-01,  1.1126e-01, -1.8291e-01,  2.7646e-01, -4.2367e-03,\n",
            "         -1.6538e-01,  6.1251e-02,  3.4077e-01,  2.9962e-01, -6.4966e-02,\n",
            "          2.9212e-01,  2.4515e-01,  1.3189e-01,  7.8509e-01, -9.8360e-02,\n",
            "         -2.9642e-01,  4.1664e-01,  5.5359e-01, -1.7591e-02, -3.8180e-01,\n",
            "         -4.0747e-01, -2.7307e-02, -3.2798e-01, -3.8704e-02,  7.5521e-02,\n",
            "         -7.8392e-02,  1.0738e-02,  2.0570e-01,  2.5741e-01,  2.1917e-01,\n",
            "          2.0606e-02,  9.2412e-02,  8.1890e-02,  7.4505e-02, -4.1632e-01,\n",
            "          1.6509e-01,  2.3960e-01,  1.2047e-01,  2.8060e-01,  1.4348e-01,\n",
            "          4.3230e-01,  1.6742e-01,  4.0665e-01,  1.1746e-01,  4.5878e-01,\n",
            "          3.7867e-02, -1.7758e-01, -2.2634e+00,  2.2676e-01,  4.3222e-01,\n",
            "          2.4111e-01, -1.8280e-01,  4.7433e-03,  6.6094e-01, -5.5127e-01,\n",
            "         -3.4875e-01, -5.5316e-01,  1.3295e-01,  2.9534e-01,  6.6606e-01,\n",
            "         -3.6882e-02, -1.2053e-01,  1.5246e-01,  2.5672e-01,  3.3924e-02,\n",
            "         -2.8932e-01,  2.3364e-02, -3.2534e-01, -4.0939e-01, -1.6131e-01,\n",
            "         -3.8661e-01, -4.5728e-01,  3.7809e-01,  3.9525e-02, -2.7045e-01,\n",
            "         -1.6933e-01,  4.5394e-01, -2.0633e-01,  2.9165e-01, -2.4695e-02,\n",
            "          2.2358e-01, -8.6458e-02, -8.4510e-02, -2.6167e-01,  1.2852e-01,\n",
            "          1.7893e-01, -3.5216e-02, -4.5276e-01,  7.2818e-01,  5.2376e-02,\n",
            "          1.9499e-02, -2.5925e-01,  1.3387e-01,  3.8803e-01, -3.5313e-01,\n",
            "          6.8203e-01, -5.4136e-01, -3.9862e-01,  8.4672e-02,  1.6223e-01,\n",
            "         -2.1902e-01, -2.2479e-01,  5.2771e-01,  2.5479e-01,  3.7325e-02,\n",
            "         -1.7531e-01, -8.2137e-01, -5.5848e-02,  5.4475e-01, -3.1591e-01,\n",
            "          4.0311e-01,  2.0231e-01, -2.5937e-01, -3.2753e-01,  2.6841e-01,\n",
            "         -5.3545e-01, -6.4268e-01, -1.9218e-01, -3.6025e-01,  4.3796e-01,\n",
            "          5.4345e-03,  6.1122e-02, -2.2438e-02, -2.2025e-01,  6.9867e-01,\n",
            "          2.6471e-01, -4.4860e-02,  1.0049e-01, -1.6045e-01, -2.2859e-01,\n",
            "         -5.3567e-02,  1.7580e-01, -5.7291e+00, -2.7269e-01, -2.0437e-01,\n",
            "         -3.0754e-01, -1.5163e-01, -4.4324e-01, -1.0407e-01, -1.4392e-01,\n",
            "         -9.7000e-02, -9.6170e-02, -1.1287e-01, -1.0222e-01, -1.6110e-01,\n",
            "         -2.2871e-01,  5.4921e-01,  3.5018e-01]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Embedding for 'dog':\n",
            "torch.Size([1, 768])\n",
            "tensor([[-5.3666e-01, -1.3823e-01, -6.1680e-01, -1.8331e-01, -2.9757e-01,\n",
            "          1.6610e-01,  4.7679e-01,  3.5779e-01, -1.8738e-01, -2.0136e-01,\n",
            "         -3.1851e-01, -1.1057e-01, -4.2884e-01,  2.3840e-01, -3.1144e-01,\n",
            "          2.7246e-01,  9.6259e-02,  5.2824e-01, -5.0031e-02,  2.8638e-01,\n",
            "         -1.1439e-01, -5.7802e-01,  1.7688e-01,  7.9285e-03,  4.3275e-01,\n",
            "          6.0532e-02,  2.7308e-01,  4.7060e-01,  2.7719e-01, -5.1663e-02,\n",
            "          9.4060e-02,  2.3778e-02, -1.0083e-01, -4.3432e-01,  8.7536e-02,\n",
            "          1.2069e-01,  8.0930e-02, -3.0423e-01,  1.2667e-01,  2.2066e-01,\n",
            "          1.6799e-02,  3.9574e-01, -2.3154e-02,  3.2695e-01, -4.7735e-02,\n",
            "         -2.2871e-01, -3.2367e+00, -1.2524e-01, -2.6412e-01, -7.2986e-02,\n",
            "          3.9012e-02,  1.6054e-01,  5.8077e-03,  7.2773e-01,  3.2951e-01,\n",
            "          1.2208e-01, -5.2001e-01, -5.2399e-02, -1.5794e-01, -7.5021e-03,\n",
            "         -7.8995e-02,  3.5495e-01, -1.7011e-01, -4.6223e-02, -1.5758e-01,\n",
            "          2.7106e-01, -1.3994e-01,  1.4123e-01, -5.1462e-01,  6.2034e-01,\n",
            "         -4.5964e-01, -1.4002e-01,  3.5589e-02,  1.9744e-01, -3.8943e-02,\n",
            "          1.4762e-01, -1.3588e-01,  6.0306e-02, -2.2740e-01,  2.2985e-01,\n",
            "          1.6566e-01,  6.3241e-01,  4.3791e-01,  1.4738e-01, -1.9002e-01,\n",
            "          1.3819e-01, -8.2043e-02, -1.5296e-01, -1.5692e-01,  4.5114e-01,\n",
            "         -1.4584e-01, -1.5112e-01, -1.7127e-01,  1.8640e-01,  4.0843e-01,\n",
            "         -3.8530e-02,  8.6117e-02, -5.8228e-02,  1.0031e-01,  4.6806e-02,\n",
            "          3.3614e-01, -4.1252e-01,  4.0221e-01, -2.5680e-01, -3.2288e-02,\n",
            "         -1.7247e-01, -1.5590e-01, -6.0885e-01, -2.7394e-01, -1.6662e+00,\n",
            "         -9.1811e-02,  1.9784e-01,  1.1921e-02, -6.0619e-01, -2.4057e-01,\n",
            "          4.8398e-01,  2.3360e-01,  1.1230e-01, -1.8437e-01, -1.7746e-01,\n",
            "          1.5601e-01, -2.6155e-01, -1.2694e-01, -1.8015e-01, -9.8134e-02,\n",
            "          1.5756e-01,  1.0866e-02, -3.0759e-01,  3.5405e-01,  1.3252e-01,\n",
            "          6.5043e-01,  5.0902e-01,  1.6945e-01, -1.1488e-01, -5.9263e-01,\n",
            "          4.5899e-01,  1.5832e-01, -2.1966e-01, -2.7008e-01,  2.7803e-01,\n",
            "         -6.8338e-01, -3.1465e-01, -2.5533e+00, -5.2064e-02,  7.7520e-01,\n",
            "          2.4695e-01,  4.1250e-01, -2.7892e-01, -1.7693e-01, -6.4944e-02,\n",
            "          1.2743e-01,  2.4330e-01, -7.4956e-02,  4.2159e-02, -2.6764e-01,\n",
            "         -5.0246e-01, -7.6987e-02, -1.7278e-01,  1.3708e-01,  7.1843e-02,\n",
            "          2.2372e-01, -2.2731e-01, -5.7783e-01, -2.7603e-01, -1.9995e-01,\n",
            "          2.7148e-01,  3.7166e-01,  1.0339e-01, -3.2024e-01,  3.2727e-01,\n",
            "          2.7043e-01,  7.1655e-02,  1.1232e+00, -4.2972e-02,  4.6995e-01,\n",
            "         -7.0991e-02,  6.1394e-02,  2.4830e-02,  5.8934e-01, -1.0469e-01,\n",
            "         -3.1478e-01,  4.3386e-01,  3.4439e-01,  1.3513e-01,  3.9318e-02,\n",
            "         -4.7636e-01,  1.3011e-01, -6.2419e-01, -4.1185e-01, -4.6333e-02,\n",
            "         -1.1810e-02, -3.1441e-01, -2.0641e-01,  3.7005e-01,  4.8219e-01,\n",
            "         -7.0145e-02, -1.8417e-01, -6.4223e-01,  1.0899e-01,  5.1110e-02,\n",
            "         -1.4745e-01, -2.1262e-01,  1.5961e-02,  1.3728e-01, -5.9764e-01,\n",
            "          3.3176e+00,  1.0012e-01, -3.4200e-01,  5.0054e-01,  4.6077e-01,\n",
            "          1.0866e-02, -1.0493e-01, -3.2643e-01, -1.6570e-01,  3.4496e-01,\n",
            "          1.1106e-01,  1.1900e-01,  2.5643e-01, -6.2696e-02, -2.8804e-01,\n",
            "         -2.0560e-02,  2.0987e-01, -4.4213e-01,  1.5541e-01, -8.2953e-02,\n",
            "          2.8932e-01,  3.2107e-01,  2.2496e-01,  2.6979e-01, -1.3564e+00,\n",
            "          3.6766e-01,  7.4507e-02, -1.1694e-01,  9.9502e-03, -6.1339e-01,\n",
            "         -2.1382e-01, -1.0721e-01, -9.9875e-02,  3.1847e-01, -2.1169e-01,\n",
            "         -4.1873e-01,  9.1797e-02,  4.5171e-01,  1.6995e-02, -3.0601e-03,\n",
            "         -2.6695e-02,  2.7281e-01,  4.3424e-01, -4.4463e-02,  1.4310e-01,\n",
            "          3.6144e-01,  3.9263e-01,  4.3104e-01, -1.6469e-01,  5.7332e-01,\n",
            "         -9.3707e-02,  2.4258e-03,  1.2637e-01, -2.7944e-01, -3.8766e-01,\n",
            "         -1.5738e-01,  1.6133e-01, -2.6012e-02,  3.0755e-02, -7.0526e-01,\n",
            "         -1.8234e-01,  3.5723e-01,  3.9524e-02,  7.4320e-02,  6.5328e-02,\n",
            "          2.1526e-01, -4.2407e-02, -6.9524e-01, -2.8022e+00, -1.3216e-01,\n",
            "         -1.3448e-01,  4.6517e-01,  5.4098e-02,  2.9349e-01, -2.2422e-01,\n",
            "          1.8819e-01,  2.6518e-01, -3.3349e-01,  4.0908e-01, -9.6556e-02,\n",
            "         -2.3639e-01,  3.8289e-01, -8.1193e-01,  5.1556e-01, -1.1748e-01,\n",
            "         -6.1193e-01,  5.1598e-02, -2.3094e-01,  1.2568e-01, -2.6228e-01,\n",
            "         -2.1472e-02,  4.8019e-02,  2.6732e-01,  5.2204e-01, -7.7823e-01,\n",
            "         -4.8293e-01, -3.9232e-01,  2.1895e-01,  2.3962e-01, -5.9657e-02,\n",
            "          7.5473e-02, -2.7540e-01, -5.5921e-01, -3.0663e+00,  4.8880e-01,\n",
            "          1.4681e-01, -4.9935e-01,  3.4164e-01,  1.8655e-02,  3.1542e-01,\n",
            "         -8.9165e-02, -4.5978e-01,  4.1816e-01,  3.2933e-01, -1.4184e-01,\n",
            "          2.5354e-01, -1.2646e-01,  2.4097e-01, -6.0568e-01,  1.8233e-01,\n",
            "         -4.2192e-01, -3.7868e-03, -1.0830e-01,  3.7147e-01, -1.4684e-01,\n",
            "         -4.7668e-01, -2.9480e-02,  3.3156e-01,  2.8178e-01, -3.2007e-01,\n",
            "         -4.8307e-01, -2.0119e-01, -4.6936e-01,  1.6462e-01, -3.7520e-01,\n",
            "         -2.4590e-01,  1.4354e-01, -5.3074e-02, -2.6947e-02, -2.6191e-02,\n",
            "          1.8285e-01,  3.0793e-01,  6.7889e-02,  9.1741e-02,  7.5832e-01,\n",
            "          3.9730e-01,  1.1710e-01,  9.2128e-01,  2.0402e-01,  4.4466e-01,\n",
            "         -5.6238e-01, -7.1157e-03,  1.7687e-01, -1.1837e-01,  2.0452e-01,\n",
            "          8.7006e-01,  4.7710e-01,  4.8638e-01, -3.1606e-01,  6.2116e-01,\n",
            "         -6.2638e-02, -3.6122e-01,  1.5829e-01,  6.9278e-01, -2.4826e-01,\n",
            "          3.0432e-02,  5.4313e-02,  3.2992e-01, -2.5209e-01, -5.2533e-02,\n",
            "         -4.7375e-01,  1.0147e-01,  6.8297e-02,  1.5615e-01,  6.3310e-01,\n",
            "         -1.6471e-01, -9.9991e-01, -6.9922e-02, -5.9526e-01,  1.6200e-01,\n",
            "         -1.1241e-01, -2.6762e-01, -2.5705e-02, -5.8749e-01, -3.2598e-01,\n",
            "         -1.3696e-01,  2.0804e-01, -4.0993e-01, -5.1634e-01, -3.5475e-01,\n",
            "          3.8637e-01, -5.5359e-01, -3.5449e-01,  6.8807e-03,  6.9774e-02,\n",
            "          2.7022e-01,  5.5427e-02, -1.0001e-01, -1.4882e-01,  5.0378e-01,\n",
            "         -6.5123e-01,  6.6908e-01, -1.8009e-01,  1.5123e-01, -2.0278e-01,\n",
            "          3.1330e-02,  2.4410e-01, -1.5349e-01, -5.3949e-01, -3.0563e-01,\n",
            "          2.2348e-01,  4.1392e-01,  3.3703e-02, -3.2179e-01, -2.4029e-01,\n",
            "         -5.5781e-02,  2.0025e-01,  1.1989e+00,  4.0374e-01,  2.0365e-01,\n",
            "          4.9233e-01,  4.5393e-01,  2.5310e-01,  2.7003e-01,  7.7520e-03,\n",
            "         -1.3631e-01, -1.8044e-01,  5.7005e-02, -5.6174e-02,  1.8741e-01,\n",
            "         -2.0886e-01, -3.4391e-01, -3.9840e-01,  2.1628e-01,  8.1312e-02,\n",
            "         -3.2317e-01, -2.8307e-01,  2.5884e-01,  1.0444e-01, -5.6474e-01,\n",
            "          1.9121e-02,  3.1749e-01,  5.8596e-02,  6.4224e-03,  3.7481e-01,\n",
            "         -7.5226e-02, -3.2715e-04,  1.0510e-01,  2.7366e-01,  1.9686e-01,\n",
            "          3.6677e-02,  1.0190e-01,  5.9774e-01,  2.4041e-01,  6.3221e-03,\n",
            "          8.6304e-02, -6.1670e-01,  1.8767e-01, -1.4554e-01,  1.9407e-01,\n",
            "          2.5689e-01, -7.4169e-02, -1.7025e-01,  2.1059e-02,  2.1331e-01,\n",
            "         -1.7220e+00,  3.2265e-01,  1.3746e-02, -1.4319e-01, -8.0249e-02,\n",
            "          2.5286e-02, -5.1284e-02,  5.7432e-01,  1.3923e-01,  1.0536e-01,\n",
            "         -8.1999e-02, -3.1097e-03, -1.1636e-01, -2.1539e-01,  2.5374e-02,\n",
            "          2.2732e-01, -5.0546e-01, -3.0099e-02,  9.4650e-02, -1.4620e-01,\n",
            "         -1.5886e-01,  1.4802e-01,  4.7540e-01, -6.7855e-03,  3.1417e-01,\n",
            "          9.8033e-02, -4.4028e-03,  3.2967e-01,  6.2306e-02,  1.3302e-01,\n",
            "         -1.9232e-01, -3.0485e-01, -9.8148e-01,  4.2403e-01,  8.3869e-02,\n",
            "         -1.6381e-02,  2.2284e-01, -3.0909e-01,  4.5400e-01,  2.8034e-01,\n",
            "         -3.4816e-01, -4.7698e-02,  1.7091e-01,  4.6770e-02,  7.4603e-01,\n",
            "         -2.8465e-01, -3.6785e-01,  1.1383e-01,  7.8784e-03,  2.5774e-02,\n",
            "          2.1728e-01, -4.7675e-02,  5.1204e-02, -3.1076e-02, -1.0570e-01,\n",
            "         -1.3464e-01, -1.8285e-02,  2.0107e-01, -2.5222e-01, -2.4077e-01,\n",
            "          3.2817e-01, -2.7557e-01, -1.2692e-01,  1.8167e-01,  7.9877e-03,\n",
            "         -4.2867e-01,  2.3399e-02, -1.0566e-01,  1.8607e-02, -1.8010e-02,\n",
            "          2.3891e-01, -2.6413e-01, -3.9139e-01,  1.5276e-01, -4.5865e-02,\n",
            "         -2.0034e-01,  2.6346e-01,  7.6405e-02,  7.7980e-01,  3.2516e-01,\n",
            "          2.9147e-01,  7.6989e-02,  1.3476e-01,  3.6198e-01, -1.1545e-01,\n",
            "         -6.5052e-02, -3.3813e-01, -2.1574e-01,  3.3880e-01,  3.3937e-01,\n",
            "         -4.1339e-01,  2.5101e-01, -1.2633e-01, -2.4385e-02, -1.1756e-01,\n",
            "          1.1921e-02,  6.4629e-02,  2.6657e-01,  5.6105e-02,  1.2237e-01,\n",
            "         -3.9243e-01,  6.1273e-01,  1.7823e-01,  3.8983e-03, -2.7511e-01,\n",
            "          3.6675e-01, -4.6329e-04, -4.5994e-02, -1.0769e-01, -4.1851e-02,\n",
            "          4.1531e-01, -2.0320e-01, -7.5924e-02,  1.0841e-01, -6.2170e-02,\n",
            "         -5.2339e-01, -3.8189e-01, -1.8342e-01,  1.5122e+00,  4.3287e-01,\n",
            "          2.4671e-01,  1.9631e-01,  2.9352e-01, -1.4926e-01, -1.5863e-01,\n",
            "          1.6228e-01, -3.4133e-01,  1.4328e-01, -1.1636e-02, -3.6020e-02,\n",
            "          2.2167e-01,  2.8538e-01,  5.0027e-01,  7.8511e-02,  4.4489e-02,\n",
            "         -1.0542e-01, -3.5370e-01,  4.0139e-01, -4.5811e-01,  6.4238e-01,\n",
            "          5.6551e-01, -2.6127e-01,  3.8925e-01, -1.2638e-01, -3.3880e-02,\n",
            "         -1.6229e-01,  1.4630e-01,  1.9696e-01, -2.3081e-01,  1.8559e-01,\n",
            "          1.5711e-01,  8.8034e-02, -6.4534e-02,  1.7591e-01,  1.2053e-01,\n",
            "          1.2127e-02,  6.9093e-02,  2.7916e-01,  2.1568e-01, -3.0535e-01,\n",
            "          2.1095e-01,  2.1779e-01,  2.4480e-01,  6.9625e-01, -2.5001e-02,\n",
            "         -4.4269e-01,  4.9854e-01,  4.7073e-01, -4.2690e-02, -4.3893e-01,\n",
            "         -3.4282e-01,  6.1605e-02, -3.3503e-01, -1.0986e-01, -1.3915e-01,\n",
            "         -8.1885e-02, -3.0120e-01,  2.3739e-01,  1.2092e-01,  2.4596e-01,\n",
            "          7.4672e-02,  1.8746e-01, -1.1570e-02,  1.0397e-01, -2.8252e-01,\n",
            "          2.1222e-01,  1.5415e-01,  6.4716e-02,  2.4896e-01,  6.7995e-02,\n",
            "          3.9969e-01,  4.6389e-01,  4.1237e-01, -1.4159e-02,  4.4833e-01,\n",
            "          1.1852e-01, -2.1526e-01, -2.1590e+00, -9.6481e-04,  4.8631e-01,\n",
            "          3.5312e-01, -1.7830e-01,  3.0775e-01,  5.7557e-01, -5.8172e-01,\n",
            "         -3.4658e-01, -5.7175e-01,  1.5664e-01,  2.2860e-01,  8.5373e-01,\n",
            "         -2.6853e-01, -1.5582e-01,  1.4580e-01,  2.8603e-01,  2.3075e-01,\n",
            "         -3.1718e-01, -7.3465e-02, -4.2605e-01, -4.2521e-01, -1.3202e-01,\n",
            "         -3.2241e-01, -6.1612e-01,  1.9864e-01,  1.5847e-01, -2.6234e-01,\n",
            "         -1.9484e-01,  3.7186e-01, -3.5017e-01,  4.0097e-01, -5.9676e-03,\n",
            "          2.1974e-01, -6.5619e-02, -2.5978e-02, -3.3631e-01,  2.3134e-01,\n",
            "          1.6782e-01,  6.4672e-02, -5.3224e-01,  6.9078e-01,  4.9121e-02,\n",
            "          5.2079e-02, -2.5289e-01,  1.4719e-01,  4.6152e-01, -3.6921e-02,\n",
            "          5.5630e-01, -4.7849e-01, -3.0426e-01,  1.0870e-01,  7.9787e-03,\n",
            "         -1.4499e-01,  9.8252e-03,  4.2155e-01,  3.5075e-01,  9.0856e-02,\n",
            "         -3.7302e-01, -7.4965e-01, -2.6098e-01,  7.1403e-01, -1.8405e-01,\n",
            "          2.5753e-01,  1.9647e-01, -3.4475e-01, -1.9019e-01,  3.3012e-01,\n",
            "         -6.9964e-01, -5.7777e-01, -9.0239e-02, -4.1597e-01,  5.3617e-01,\n",
            "         -1.9912e-01, -5.4630e-02,  1.2827e-01, -2.4146e-01,  7.5029e-01,\n",
            "          1.8670e-01,  1.1777e-01,  2.9903e-02, -1.3557e-01, -3.1440e-01,\n",
            "         -1.6380e-01,  1.9520e-01, -5.4863e+00, -1.9693e-01, -2.4775e-01,\n",
            "         -1.9301e-01, -4.6433e-02, -3.8679e-01, -9.5659e-02, -1.9275e-01,\n",
            "          2.6958e-02, -1.7610e-01, -1.4352e-01, -1.3660e-01,  7.6265e-02,\n",
            "         -2.2541e-01,  3.4907e-01,  4.2951e-01]], grad_fn=<SelectBackward0>)\n",
            "Text Encoder tested successfully with example inputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c12af0d"
      },
      "source": [
        "## Implement Generative Backend\n",
        "\n",
        "### Subtask:\n",
        "Implement the Generative Backend consisting of a VQ-VAE and a Transformer. The VQ-VAE will encode and decode image patches, and the Transformer will learn to generate sequences of VQ-VAE codes from the text embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696d7f94"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin implementing the VQ-VAE module, I need to import the essential libraries such as `torch` for tensor operations and `torch.nn` for building neural network layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc41f909",
        "outputId": "bb4423b2-47d0-4308-cd8b-a89407dcb583"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"PyTorch and nn modules imported successfully.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch and nn modules imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2648bd93"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the `VectorQuantizer` class, which is a crucial part of the VQ-VAE. This class will handle the quantization of the latent embeddings by mapping them to the closest codebook entries and computing the VQ loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b226c3ec",
        "outputId": "f3596f6e-07ee-4942-c4c1-f098e36c0c49"
      },
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
        "        self.embedding.weight.data.uniform_(-1 / self.num_embeddings, 1 / self.num_embeddings)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Reshape inputs to (batch_size, num_vectors, embedding_dim)\n",
        "        # inputs shape: (batch, C, H, W) -> (batch, H*W, C)\n",
        "        flat_inputs = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        # distances shape: (batch*H*W, num_embeddings)\n",
        "        distances = (torch.sum(flat_inputs**2, dim=1, keepdim=True)\n",
        "                     + torch.sum(self.embedding.weight**2, dim=1)\n",
        "                     - 2 * torch.matmul(flat_inputs, self.embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        # encoding_indices shape: (batch*H*W,)\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        # encodings shape: (batch*H*W, num_embeddings) (one-hot)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        # quantized shape: (batch*H*W, embedding_dim)\n",
        "        quantized = torch.matmul(encodings, self.embedding.weight).view(inputs.shape)\n",
        "\n",
        "        # Loss\n",
        "        # commitment loss: how far the encoder output is from the chosen codebook vector\n",
        "        # embedding loss: how far the chosen codebook vector is from the encoder output\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = inputs + (quantized - inputs).detach() # Straight-through estimator\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return loss, quantized, perplexity, encodings\n",
        "\n",
        "print(\"VectorQuantizer class defined.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorQuantizer class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b6d4b03"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `VectorQuantizer` class is defined, I will implement the Encoder part of the VQ-VAE. This will involve defining a convolutional neural network architecture that downsamples the input image into a latent representation that can then be fed into the `VectorQuantizer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba31a8ed",
        "outputId": "b2b36d42-e9d7-421d-d306-e2f02fa41122"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_channels, num_hiddens // 2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv_2 = nn.Conv2d(num_hiddens // 2, num_hiddens, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv_3 = nn.Conv2d(num_hiddens, num_hiddens, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.residual_stack = nn.Sequential(\n",
        "            *[ResidualBlock(num_hiddens, num_residual_hiddens) for _ in range(num_residual_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = F.relu(self.conv_1(inputs))\n",
        "        x = F.relu(self.conv_2(x))\n",
        "        x = F.relu(self.conv_3(x))\n",
        "        return self.residual_stack(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(in_channels, num_hiddens, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_2 = nn.Conv2d(num_hiddens, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.conv_1(x))\n",
        "        h = self.conv_2(h)\n",
        "        return x + h\n",
        "\n",
        "print(\"Encoder and ResidualBlock classes defined.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder and ResidualBlock classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e531e25"
      },
      "source": [
        "**Reasoning**:\n",
        "With the `Encoder` and `VectorQuantizer` defined, the next step is to implement the `Decoder` class. This class will take the quantized latent representations and reconstruct the original image from them, completing the VQ-VAE architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d3c1dea",
        "outputId": "8858ba3b-3329-4e98-93b9-93b3fc3b7843"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_channels, num_hiddens, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.residual_stack = nn.Sequential(\n",
        "            *[ResidualBlock(num_hiddens, num_residual_hiddens) for _ in range(num_residual_layers)]\n",
        "        )\n",
        "\n",
        "        self.conv_trans_1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 2,\n",
        "                                                 kernel_size=4, stride=2, padding=1)\n",
        "        self.conv_trans_2 = nn.ConvTranspose2d(num_hiddens // 2, 1,\n",
        "                                                 kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.residual_stack(x)\n",
        "        x = F.relu(self.conv_trans_1(x))\n",
        "        return self.conv_trans_2(x)\n",
        "\n",
        "print(\"Decoder class defined.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d255c1b6"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Encoder, Decoder, and VectorQuantizer classes are defined, I will integrate them into a complete VQVAE module. This class will encapsulate the entire VQ-VAE architecture, handling the forward pass from input image to reconstructed image and quantized latent codes, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4f32ef",
        "outputId": "247df919-ef06-4c54-e5f7-ed8ad57b4f9e"
      },
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VQVAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(in_channels, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "        self.pre_vq_conv = nn.Conv2d(num_hiddens, embedding_dim, kernel_size=1, stride=1)\n",
        "        self.vq = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
        "        self.post_vq_conv = nn.Conv2d(embedding_dim, num_hiddens, kernel_size=1, stride=1)\n",
        "        self.decoder = Decoder(num_hiddens, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        z = self.encoder(inputs)\n",
        "        z = self.pre_vq_conv(z)\n",
        "        loss, quantized, perplexity, _ = self.vq(z)\n",
        "        reconstructions = self.decoder(self.post_vq_conv(quantized))\n",
        "        return loss, reconstructions, perplexity\n",
        "\n",
        "print(\"VQVAE class defined, integrating Encoder, VectorQuantizer, and Decoder.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VQVAE class defined, integrating Encoder, VectorQuantizer, and Decoder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cd7162"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the VQ-VAE module is complete, the next step is to implement the Transformer model, which will take text embeddings and generate VQ-VAE codes. I'll start by defining the MultiHeadSelfAttention mechanism, a core component of a Transformer, to allow the model to weigh the importance of different parts of the input sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8757de7",
        "outputId": "f2e104bb-9256-4cf3-fa56-6c7cc002b2c8"
      },
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.queries = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.keys = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.values = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.fc_out = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        N = query.shape[0]\n",
        "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
        "\n",
        "        queries = self.queries(query).view(N, query_len, self.num_heads, self.head_dim)\n",
        "        keys = self.keys(key).view(N, key_len, self.num_heads, self.head_dim)\n",
        "        values = self.values(value).view(N, value_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        queries = queries.permute(0, 2, 1, 3) # (N, num_heads, query_len, head_dim)\n",
        "        keys = keys.permute(0, 2, 1, 3)    # (N, num_heads, key_len, head_dim)\n",
        "        values = values.permute(0, 2, 1, 3)  # (N, num_heads, value_len, head_dim)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        attention_scores = torch.matmul(queries, keys.permute(0, 1, 3, 2)) / (self.embed_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = F.softmax(attention_scores, dim=-1)\n",
        "        out = torch.matmul(attention, values)\n",
        "\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(N, query_len, self.embed_dim)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "print(\"MultiHeadSelfAttention class defined.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadSelfAttention class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71479db2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `MultiHeadSelfAttention` module is defined, I will implement a `TransformerBlock`. This block is a fundamental component of the Transformer architecture, combining multi-head self-attention with a feed-forward network, layer normalization, and dropout to process sequential data, in this case, text embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d713e8b",
        "outputId": "149d621e-2ddf-4f24-d481-a1ad814ff8f3"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask=None):\n",
        "        attention = self.attention(query, key, value, mask)\n",
        "        x = self.norm1(query + self.dropout(attention))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm2(x + self.dropout(forward))\n",
        "        return out\n",
        "\n",
        "print(\"TransformerBlock class defined.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerBlock class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "818d1714"
      },
      "source": [
        "**Reasoning**:\n",
        "With the `TransformerBlock` defined, I will now implement the full Transformer model. This model will incorporate multiple `TransformerBlock`s, positional embeddings, and a final linear layer to project the Transformer's output to the VQ-VAE code space, taking semantic embeddings as input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93717585",
        "outputId": "2b065f5a-2eaf-4a87-ff9b-499f6967a561"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, text_embed_dim, num_vq_embeddings, image_code_sequence_length, embed_dim, num_heads, num_layers, ff_dim, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.token_embedding = nn.Linear(text_embed_dim, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(image_code_sequence_length, embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.to_vq_codes = nn.Linear(embed_dim, num_vq_embeddings)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        # text_embedding: (batch_size, text_embed_dim)\n",
        "\n",
        "        # Expand text embedding to a sequence, e.g., for the first token or as initial context\n",
        "        # For simplicity, we'll repeat the text embedding for each position in the output sequence\n",
        "        # A more sophisticated approach might use a single 'start' token and condition the generation.\n",
        "\n",
        "        # Let's assume the transformer will generate `image_code_sequence_length` VQ codes\n",
        "        batch_size = text_embedding.shape[0]\n",
        "        device = text_embedding.device\n",
        "\n",
        "        # Project text_embedding to the transformer's embed_dim\n",
        "        x = self.token_embedding(text_embedding).unsqueeze(1) # (batch_size, 1, embed_dim)\n",
        "\n",
        "        # Initialize a sequence of learnable 'code tokens' or just use the text embedding\n",
        "        # For now, let's assume we want to generate a sequence of fixed length.\n",
        "        # The transformer needs a sequence to process. Let's make it autoregressive.\n",
        "        # For initial simple implementation, we'll just use the text embedding as the query for all positions\n",
        "        # and let the transformer output a sequence. This is a simplification.\n",
        "\n",
        "        # A more standard approach would be to have a sequence of target embeddings\n",
        "        # and condition on the text embedding.\n",
        "\n",
        "        # Let's create dummy input sequence for now, representing the positions to generate\n",
        "        # (batch_size, image_code_sequence_length, embed_dim)\n",
        "        target_sequence = torch.zeros(batch_size, self.position_embedding.num_embeddings, self.token_embedding.out_features, device=device)\n",
        "        positions = torch.arange(0, self.position_embedding.num_embeddings, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "        target_sequence += self.position_embedding(positions)\n",
        "\n",
        "        # For a decoder-only transformer (common for generation), query, key, value are all from target_sequence\n",
        "        # but we also want to condition on the text_embedding.\n",
        "        # One way is to prepend the text_embedding to the target sequence.\n",
        "\n",
        "        # Let's simplify and make the text_embedding serve as the initial context\n",
        "        # and the transformer outputs a sequence of code embeddings.\n",
        "        # This is a common pattern for conditional generation.\n",
        "\n",
        "        # The `TransformerBlock` expects value, key, query. Here, the text embedding will serve as key/value\n",
        "        # for the cross-attention, and the positional embeddings as query for self-attention on the output sequence.\n",
        "        # However, the current TransformerBlock is a standard encoder block that uses self-attention (query, key, value all same).\n",
        "        # For generation, we typically need a decoder structure with masked self-attention and cross-attention.\n",
        "\n",
        "        # Let's adapt to a common decoder structure where text_embedding is K, V for cross-attention, and\n",
        "        # the generated sequence tokens are Q for self-attention. Our current TransformerBlock is only self-attention.\n",
        "\n",
        "        # Re-thinking: A simpler way to integrate text_embedding for a *sequence-to-sequence* like transformer\n",
        "        # where text_embedding influences each block's output.\n",
        "        # Or, we can use the text embedding as the initial state for the decoder's sequence generation.\n",
        "\n",
        "        # For this subtask, the instruction says \"Transformer will learn to generate sequences of VQ-VAE codes from the text embeddings.\"\n",
        "        # Let's make it a simple conditional transformer for now.\n",
        "\n",
        "        # Create an initial input sequence for the transformer, based on text_embedding and positional encoding.\n",
        "        # We need a sequence of length `image_code_sequence_length` to output VQ codes.\n",
        "        # One simple way is to use the `text_embedding` (expanded) as the input to the first block,\n",
        "        # combined with positional embeddings.\n",
        "\n",
        "        # Reshape text_embedding to be compatible with a sequence input, e.g., by replicating it\n",
        "        # (batch_size, 1, embed_dim) -> (batch_size, image_code_sequence_length, embed_dim)\n",
        "        # A common approach is to use a learnable 'start' token and then generate tokens one by one.\n",
        "        # Given the instruction \"generate sequences of VQ-VAE codes from the text embeddings\",\n",
        "        # let's assume we have a fixed-length output sequence to generate.\n",
        "\n",
        "        # Let's create an input sequence for the transformer that is conditioned on the text embedding.\n",
        "        # The input tokens for the transformer can be derived from the text embedding plus positional encoding.\n",
        "\n",
        "        # Initialize the input sequence for the transformer with the projected text embedding\n",
        "        # and add positional embeddings for a fixed output length (e.g., corresponding to the latent image size).\n",
        "\n",
        "        conditioned_input = self.token_embedding(text_embedding).unsqueeze(1) # (batch_size, 1, embed_dim)\n",
        "\n",
        "        # If we need to generate a sequence of length L, we need an input sequence of length L.\n",
        "        # A simpler approach for *generating* a sequence from a single embedding is to use the text embedding\n",
        "        # as a constant context, and have the transformer generate tokens, potentially autoregressively.\n",
        "        # For a non-autoregressive generation, we can generate all at once.\n",
        "\n",
        "        # Let's prepare a sequence of length `image_code_sequence_length` for the transformer.\n",
        "        # Each element of this sequence will be the text embedding + its positional embedding.\n",
        "\n",
        "        # Create a sequence of text embeddings + positional embeddings\n",
        "        # (batch_size, image_code_sequence_length, embed_dim)\n",
        "        initial_sequence_input = self.token_embedding(text_embedding).unsqueeze(1).repeat(1, self.position_embedding.num_embeddings, 1)\n",
        "        positions = torch.arange(0, self.position_embedding.num_embeddings, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "        initial_sequence_input += self.position_embedding(positions)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        out = initial_sequence_input\n",
        "        for block in self.transformer_blocks:\n",
        "            out = block(out, out, out) # Self-attention\n",
        "\n",
        "        # Project to VQ code logits\n",
        "        vq_code_logits = self.to_vq_codes(out) # (batch_size, image_code_sequence_length, num_vq_embeddings)\n",
        "\n",
        "        return vq_code_logits\n",
        "\n",
        "print(\"Transformer class defined.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e7cf0eb"
      },
      "source": [
        "## Assemble Complete Model\n",
        "\n",
        "### Subtask:\n",
        "Combine the Text Encoder and the Generative Backend to create the full text-to-silhouette model. Ensure the output of the Text Encoder correctly feeds into the Transformer component of the Generative Backend, keeping the total parameter count under 250 million.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a188e62e"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a new class `TextToSilhouetteModel` that integrates the Text Encoder (DistilBERT) and the Generative Backend (VQ-VAE and Transformer) as specified in the instructions. This class will handle the entire forward pass from text input to silhouette output and manage the total parameter count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f63ee25",
        "outputId": "f7c286ac-1c55-4a89-94af-04823c75e328"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "# Assuming VQVAE, Transformer, Encoder, Decoder, ResidualBlock, MultiHeadSelfAttention, VectorQuantizer classes are already defined and available\n",
        "\n",
        "class TextToSilhouetteModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 # VQ-VAE parameters\n",
        "                 vqvae_in_channels=1, vqvae_num_hiddens=128, vqvae_num_residual_layers=2,\n",
        "                 vqvae_num_residual_hiddens=64, vqvae_num_embeddings=512,\n",
        "                 vqvae_embedding_dim=64, vqvae_commitment_cost=0.25,\n",
        "                 # Transformer parameters\n",
        "                 text_embed_dim=768, transformer_embed_dim=256, transformer_num_heads=8,\n",
        "                 transformer_num_layers=4, transformer_ff_dim=1024, transformer_dropout=0.1):\n",
        "        super(TextToSilhouetteModel, self).__init__()\n",
        "\n",
        "        # 1. Text Encoder (DistilBERT)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # 2. VQ-VAE (Generative Backend part 1)\n",
        "        self.vqvae = VQVAE(in_channels=vqvae_in_channels,\n",
        "                           num_hiddens=vqvae_num_hiddens,\n",
        "                           num_residual_layers=vqvae_num_residual_layers,\n",
        "                           num_residual_hiddens=vqvae_num_residual_hiddens,\n",
        "                           num_embeddings=vqvae_num_embeddings,\n",
        "                           embedding_dim=vqvae_embedding_dim,\n",
        "                           commitment_cost=vqvae_commitment_cost)\n",
        "\n",
        "        # Calculate latent spatial dimensions for 128x128 image with Encoder's downsampling (4x)\n",
        "        # Encoder has two conv layers with stride 2 each, so total downsampling is 2*2 = 4\n",
        "        self.latent_H = 128 // (2*2) # 32\n",
        "        self.latent_W = 128 // (2*2) # 32\n",
        "        image_code_sequence_length = self.latent_H * self.latent_W # 32*32 = 1024\n",
        "\n",
        "        # 3. Transformer (Generative Backend part 2)\n",
        "        self.transformer = Transformer(text_embed_dim=text_embed_dim,\n",
        "                                       num_vq_embeddings=vqvae_num_embeddings, # Output logits for VQ-VAE codebook entries\n",
        "                                       image_code_sequence_length=image_code_sequence_length,\n",
        "                                       embed_dim=transformer_embed_dim,\n",
        "                                       num_heads=transformer_num_heads,\n",
        "                                       num_layers=transformer_num_layers,\n",
        "                                       ff_dim=transformer_ff_dim,\n",
        "                                       dropout=transformer_dropout)\n",
        "\n",
        "    def forward(self, word, meaning):\n",
        "        # Generate text embedding using DistilBERT\n",
        "        text = f\"{word}: {meaning}\"\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "        # Ensure inputs are on the same device as the model\n",
        "        inputs = {k: v.to(self.text_encoder.device) for k, v in inputs.items()}\n",
        "        text_encoder_outputs = self.text_encoder(**inputs)\n",
        "        text_embedding = text_encoder_outputs.last_hidden_state[:, 0, :] # (batch_size, text_embed_dim)\n",
        "\n",
        "        # Pass text embedding to Transformer to get VQ code logits\n",
        "        vq_code_logits = self.transformer(text_embedding)\n",
        "        # vq_code_logits shape: (batch_size, image_code_sequence_length, num_vq_embeddings)\n",
        "\n",
        "        # Convert logits to discrete code indices\n",
        "        # argmax along the last dimension gives the index of the most probable codebook entry for each position\n",
        "        # (batch_size, image_code_sequence_length)\n",
        "        vq_code_indices = torch.argmax(vq_code_logits, dim=-1)\n",
        "\n",
        "        # Look up quantized embeddings from VQ-VAE's codebook\n",
        "        # self.vqvae.vq.embedding.weight has shape (num_embeddings, embedding_dim)\n",
        "        # Using F.embedding to get (batch_size, image_code_sequence_length, embedding_dim)\n",
        "        quantized_embeddings_flat = F.embedding(vq_code_indices, self.vqvae.vq.embedding.weight)\n",
        "\n",
        "        # Reshape to (batch_size, embedding_dim, latent_H, latent_W) for VQ-VAE decoder\n",
        "        quantized_embeddings_reshaped = quantized_embeddings_flat.view(\n",
        "            text_embedding.shape[0], self.latent_H, self.latent_W, self.vqvae.vq.embedding_dim\n",
        "        ).permute(0, 3, 1, 2) # (batch_size, embedding_dim, latent_H, latent_W)\n",
        "\n",
        "        # Pass reshaped latent embeddings through VQ-VAE's post_vq_conv and decoder\n",
        "        # The post_vq_conv layer expects (batch_size, embedding_dim, latent_H, latent_W)\n",
        "        # Its output is (batch_size, num_hiddens, latent_H, latent_W)\n",
        "        decoder_input = self.vqvae.post_vq_conv(quantized_embeddings_reshaped)\n",
        "        reconstructed_silhouette = self.vqvae.decoder(decoder_input)\n",
        "\n",
        "        return reconstructed_silhouette\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = TextToSilhouetteModel()\n",
        "\n",
        "# Calculate total trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"TextToSilhouetteModel instantiated.\")\n",
        "print(f\"Total trainable parameters: {total_params / 1e6:.2f} million\")\n",
        "\n",
        "# Check if parameters are under 250 million\n",
        "if total_params < 250e6:\n",
        "    print(\"Parameter count is within the limit (under 250 million).\")\n",
        "else:\n",
        "    print(\"WARNING: Parameter count exceeds the limit of 250 million.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextToSilhouetteModel instantiated.\n",
            "Total trainable parameters: 71.05 million\n",
            "Parameter count is within the limit (under 250 million).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee65cbf"
      },
      "source": [
        "## Prepare Training Dataset\n",
        "\n",
        "### Subtask:\n",
        "Prepare a dataset of word-meaning pairs and corresponding silhouette images suitable for training the model. This dataset should be structured to allow the model to learn the mapping from text to visual representation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110f160f"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin preparing the training dataset, I will import the necessary libraries: `torch.utils.data` for dataset management, `torchvision.transforms` for image preprocessing, and `PIL` for image loading, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17d96cea",
        "outputId": "6e08234f-77f8-4729-a239-ba9ad3122065"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Necessary libraries for dataset preparation imported successfully.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary libraries for dataset preparation imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c200efda"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary libraries are imported, I will define a custom PyTorch `Dataset` class, `CustomSilhouetteDataset`. This class will be responsible for loading silhouette images, applying transformations, and associating them with their corresponding word-meaning pairs, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da8d700a",
        "outputId": "80d0fa3a-a681-4309-caf8-dfe434a8a2fd"
      },
      "source": [
        "class CustomSilhouetteDataset(Dataset):\n",
        "    def __init__(self, image_dir, word_meaning_df, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.word_meaning_df = word_meaning_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_meaning_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.word_meaning_df.loc[idx, 'image_filename']\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert('L') # Convert to grayscale\n",
        "\n",
        "        word = self.word_meaning_df.loc[idx, 'word']\n",
        "        meaning = self.word_meaning_df.loc[idx, 'meaning']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, word, meaning\n",
        "\n",
        "print(\"CustomSilhouetteDataset class defined.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomSilhouetteDataset class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bb5e501"
      },
      "source": [
        "**Reasoning**:\n",
        "With the `CustomSilhouetteDataset` defined, I will now create dummy data to simulate `image_dir` and `word_meaning_df`. This includes creating a temporary directory for dummy images and a pandas DataFrame for word-meaning pairs, which will allow me to instantiate the dataset and demonstrate its functionality, as required by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "433c54b5",
        "outputId": "fc760dd5-cb0a-438a-b2c0-6fed523841a5"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# 1. Create a dummy image directory and dummy images\n",
        "dummy_image_dir = 'dummy_silhouette_images'\n",
        "os.makedirs(dummy_image_dir, exist_ok=True)\n",
        "\n",
        "# Create 5 dummy grayscale images (128x128)\n",
        "num_dummy_images = 5\n",
        "image_filenames = []\n",
        "for i in range(num_dummy_images):\n",
        "    img_array = np.random.randint(0, 256, (128, 128), dtype=np.uint8)\n",
        "    img = Image.fromarray(img_array, mode='L') # 'L' for grayscale\n",
        "    filename = f'silhouette_{i+1}.png'\n",
        "    img.save(os.path.join(dummy_image_dir, filename))\n",
        "    image_filenames.append(filename)\n",
        "\n",
        "# 2. Create a dummy word_meaning_df\n",
        "dummy_data = {\n",
        "    'image_filename': image_filenames,\n",
        "    'word': ['cat', 'dog', 'house', 'tree', 'car'],\n",
        "    'meaning': [\n",
        "        'a small domesticated carnivorous mammal with soft fur, a short snout, and retractile claws.',\n",
        "        'a domesticated carnivorous mammal that typically has a long snout, an acute sense of smell, non-retractable claws, and a barking, howling, or whining voice.',\n",
        "        'a building for human habitation, especially one that is lived in by a family or small group of people.',\n",
        "        'a woody perennial plant, typically having a single stem or trunk growing to a considerable height and bearing lateral branches at some distance from the ground.',\n",
        "        'a road vehicle, typically with four wheels, powered by an internal combustion engine or electric motor and able to carry a small number of people.'\n",
        "    ]\n",
        "}\n",
        "word_meaning_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "# 3. Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
        "    transforms.Grayscale(num_output_channels=1), # Ensure grayscale\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor (scales to [0, 1])\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "print(\"Dummy image directory, images, word_meaning_df, and transformations prepared.\")\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy image directory, images, word_meaning_df, and transformations prepared.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4002848216.py:16: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  img = Image.fromarray(img_array, mode='L') # 'L' for grayscale\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8cab902"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `CustomSilhouetteDataset` class is defined and dummy data (image directory, `word_meaning_df`, and `transform`) is prepared, I will instantiate the `CustomSilhouetteDataset` and then create a `DataLoader` instance using it, as per the instructions for preparing the training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c05ef50f",
        "outputId": "e41dccee-c8a5-42ec-8488-76fd9e61a708"
      },
      "source": [
        "dataset = CustomSilhouetteDataset(image_dir=dummy_image_dir, word_meaning_df=word_meaning_df, transform=transform)\n",
        "\n",
        "batch_size = 2 # Small batch size for demonstration\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"CustomSilhouetteDataset instantiated with {len(dataset)} samples.\")\n",
        "print(f\"DataLoader instantiated with batch size {batch_size}.\")\n",
        "\n",
        "# Optional: Test fetching a batch\n",
        "for i, (images, words, meanings) in enumerate(dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"  Images shape: {images.shape}\") # Should be (batch_size, 1, 128, 128)\n",
        "    print(f\"  Words: {words}\")\n",
        "    print(f\"  Meanings: {meanings}\")\n",
        "    if i == 0: # Only print first batch for brevity\n",
        "        break\n",
        "\n",
        "# Clean up dummy directory after demonstration\n",
        "shutil.rmtree(dummy_image_dir)\n",
        "print(f\"Dummy image directory '{dummy_image_dir}' removed.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomSilhouetteDataset instantiated with 5 samples.\n",
            "DataLoader instantiated with batch size 2.\n",
            "Batch 1:\n",
            "  Images shape: torch.Size([2, 1, 128, 128])\n",
            "  Words: ('car', 'dog')\n",
            "  Meanings: ('a road vehicle, typically with four wheels, powered by an internal combustion engine or electric motor and able to carry a small number of people.', 'a domesticated carnivorous mammal that typically has a long snout, an acute sense of smell, non-retractable claws, and a barking, howling, or whining voice.')\n",
            "Dummy image directory 'dummy_silhouette_images' removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36865766"
      },
      "source": [
        "## Train Text-to-Silhouette Model\n",
        "\n",
        "### Subtask:\n",
        "Train the assembled model using the prepared dataset. Focus on achieving the target of generating 128x128 clean silhouettes and monitor the model's parameter count to stay under 250 million.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfb7bbe"
      },
      "source": [
        "**Reasoning**:\n",
        "The existing `TextToSilhouetteModel`'s `forward` method expects individual `word` and `meaning` strings, which is not suitable for batched training. To correctly process batches from the `DataLoader`, I need to modify the `forward` method to accept lists of words and meanings and handle the tokenization and text embedding generation for the entire batch efficiently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71319c5d",
        "outputId": "d83e0fe8-04e2-4f6c-a92a-69125732d7f1"
      },
      "source": [
        "class TextToSilhouetteModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 # VQ-VAE parameters\n",
        "                 vqvae_in_channels=1, vqvae_num_hiddens=128, vqvae_num_residual_layers=2,\n",
        "                 vqvae_num_residual_hiddens=64, vqvae_num_embeddings=512,\n",
        "                 vqvae_embedding_dim=64, vqvae_commitment_cost=0.25,\n",
        "                 # Transformer parameters\n",
        "                 text_embed_dim=768, transformer_embed_dim=256, transformer_num_heads=8,\n",
        "                 transformer_num_layers=4, transformer_ff_dim=1024, transformer_dropout=0.1):\n",
        "        super(TextToSilhouetteModel, self).__init__()\n",
        "\n",
        "        # 1. Text Encoder (DistilBERT)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # 2. VQ-VAE (Generative Backend part 1)\n",
        "        self.vqvae = VQVAE(in_channels=vqvae_in_channels,\n",
        "                           num_hiddens=vqvae_num_hiddens,\n",
        "                           num_residual_layers=vqvae_num_residual_layers,\n",
        "                           num_residual_hiddens=vqvae_num_residual_hiddens,\n",
        "                           num_embeddings=vqvae_num_embeddings,\n",
        "                           embedding_dim=vqvae_embedding_dim,\n",
        "                           commitment_cost=vqvae_commitment_cost)\n",
        "\n",
        "        # Calculate latent spatial dimensions for 128x128 image with Encoder's downsampling (4x)\n",
        "        # Encoder has two conv layers with stride 2 each, so total downsampling is 2*2 = 4\n",
        "        self.latent_H = 128 // (2*2) # 32\n",
        "        self.latent_W = 128 // (2*2) # 32\n",
        "        image_code_sequence_length = self.latent_H * self.latent_W # 32*32 = 1024\n",
        "\n",
        "        # 3. Transformer (Generative Backend part 2)\n",
        "        self.transformer = Transformer(text_embed_dim=text_embed_dim,\n",
        "                                       num_vq_embeddings=vqvae_num_embeddings, # Output logits for VQ-VAE codebook entries\n",
        "                                       image_code_sequence_length=image_code_sequence_length,\n",
        "                                       embed_dim=transformer_embed_dim,\n",
        "                                       num_heads=transformer_num_heads,\n",
        "                                       num_layers=transformer_num_layers,\n",
        "                                       ff_dim=transformer_ff_dim,\n",
        "                                       dropout=transformer_dropout)\n",
        "\n",
        "    def forward(self, words, meanings):\n",
        "        # Prepare batch of text for DistilBERT\n",
        "        texts = [f\"{w}: {m}\" for w, m in zip(words, meanings)]\n",
        "\n",
        "        # Tokenize and encode the entire batch\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "        # Ensure inputs are on the same device as the model\n",
        "        device = next(self.text_encoder.parameters()).device # Get current device of the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        text_encoder_outputs = self.text_encoder(**inputs)\n",
        "        # (batch_size, text_embed_dim)\n",
        "        text_embedding = text_encoder_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Pass text embedding to Transformer to get VQ code logits\n",
        "        vq_code_logits = self.transformer(text_embedding)\n",
        "        # vq_code_logits shape: (batch_size, image_code_sequence_length, num_vq_embeddings)\n",
        "\n",
        "        # Convert logits to discrete code indices\n",
        "        # argmax along the last dimension gives the index of the most probable codebook entry for each position\n",
        "        # (batch_size, image_code_sequence_length)\n",
        "        vq_code_indices = torch.argmax(vq_code_logits, dim=-1)\n",
        "\n",
        "        # Look up quantized embeddings from VQ-VAE's codebook\n",
        "        # self.vqvae.vq.embedding.weight has shape (num_embeddings, embedding_dim)\n",
        "        # Using F.embedding to get (batch_size, image_code_sequence_length, embedding_dim)\n",
        "        quantized_embeddings_flat = F.embedding(vq_code_indices, self.vqvae.vq.embedding.weight)\n",
        "\n",
        "        # Reshape to (batch_size, embedding_dim, latent_H, latent_W) for VQ-VAE decoder\n",
        "        quantized_embeddings_reshaped = quantized_embeddings_flat.view(\n",
        "            text_embedding.shape[0], self.latent_H, self.latent_W, self.vqvae.vq.embedding_dim\n",
        "        ).permute(0, 3, 1, 2) # (batch_size, embedding_dim, latent_H, latent_W)\n",
        "\n",
        "        # Pass reshaped latent embeddings through VQ-VAE's post_vq_conv and decoder\n",
        "        # The post_vq_conv layer expects (batch_size, embedding_dim, latent_H, latent_W)\n",
        "        # Its output is (batch_size, num_hiddens, latent_H, latent_W)\n",
        "        decoder_input = self.vqvae.post_vq_conv(quantized_embeddings_reshaped)\n",
        "        reconstructed_silhouette = self.vqvae.decoder(decoder_input)\n",
        "\n",
        "        return reconstructed_silhouette\n",
        "\n",
        "print(\"TextToSilhouetteModel class updated to handle batched inputs for words and meanings.\")\n",
        "\n",
        "# Re-instantiate the model to reflect the changes\n",
        "model = TextToSilhouetteModel()\n",
        "\n",
        "# Calculate total trainable parameters again, just to be sure (should be the same).\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Re-instantiated TextToSilhouetteModel.\")\n",
        "print(f\"Total trainable parameters: {total_params / 1e6:.2f} million\")\n",
        "\n",
        "if total_params < 250e6:\n",
        "    print(\"Parameter count is still within the limit (under 250 million).\")\n",
        "else:\n",
        "    print(\"WARNING: Parameter count exceeds the limit of 250 million.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextToSilhouetteModel class updated to handle batched inputs for words and meanings.\n",
            "Re-instantiated TextToSilhouetteModel.\n",
            "Total trainable parameters: 71.05 million\n",
            "Parameter count is still within the limit (under 250 million).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b22a5d6"
      },
      "source": [
        "**Reasoning**:\n",
        "To enable training of the VQ-VAE component and to align with the instruction to retrieve VQ-VAE loss from the model's forward pass, the `TextToSilhouetteModel`'s `forward` method must be modified to accept input images and return the VQ-VAE specific loss derived from those images, in addition to the reconstructed silhouette generated from text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "3b1b08e9",
        "outputId": "560a25f3-51cf-4b59-c830-2ad23bed02aa"
      },
      "source": [
        "class TextToSilhouetteModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 # VQ-VAE parameters\n",
        "                 vqvae_in_channels=1, vqvae_num_hiddens=128, vqvae_num_residual_layers=2,\n",
        "                 vqvae_num_residual_hiddens=64, vqvae_num_embeddings=512,\n",
        "                 vqvae_embedding_dim=64, vqvae_commitment_cost=0.25,\n",
        "                 # Transformer parameters\n",
        "                 text_embed_dim=768, transformer_embed_dim=256, transformer_num_heads=8,\n",
        "                 transformer_num_layers=4, transformer_ff_dim=1024, transformer_dropout=0.1):\n",
        "        super(TextToSilhouetteModel, self).__init__()\n",
        "\n",
        "        # 1. Text Encoder (DistilBERT)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # 2. VQ-VAE (Generative Backend part 1)\n",
        "        self.vqvae = VQVAE(in_channels=vqvae_in_channels,\n",
        "                           num_hiddens=vqvae_num_hiddens,\n",
        "                           num_residual_layers=vqvae_num_residual_layers,\n",
        "                           num_residual_hiddens=vqvae_num_residual_hiddens,\n",
        "                           num_embeddings=vqvae_num_embeddings,\n",
        "                           embedding_dim=vqvae_embedding_dim,\n",
        "                           commitment_cost=vqvae_commitment_cost)\n",
        "\n",
        "        # Calculate latent spatial dimensions for 128x128 image with Encoder's downsampling (4x)\n",
        "        # Encoder has two conv layers with stride 2 each, so total downsampling is 2*2 = 4\n",
        "        self.latent_H = 128 // (2*2) # 32\n",
        "        self.latent_W = 128 // (2*2) # 32\n",
        "        image_code_sequence_length = self.latent_H * self.latent_W # 32*32 = 1024\n",
        "\n",
        "        # 3. Transformer (Generative Backend part 2)\n",
        "        self.transformer = Transformer(text_embed_dim=text_embed_dim,\n",
        "                                       num_vq_embeddings=vqvae_num_embeddings, # Output logits for VQ-VAE codebook entries\n",
        "                                       image_code_sequence_length=image_code_sequence_length,\n",
        "                                       embed_dim=transformer_embed_dim,\n",
        "                                       num_heads=transformer_num_heads,\n",
        "                                       num_layers=transformer_num_layers,\n",
        "                                       ff_dim=transformer_ff_dim,\n",
        "                                       dropout=transformer_dropout)\n",
        "\n",
        "    def forward(self, images, words, meanings):\n",
        "        # Path 1: VQ-VAE loss calculation from input images\n",
        "        # This trains the VQ-VAE to correctly encode/decode images and learn its codebook\n",
        "        vq_loss, vqvae_reconstructions_from_images, _ = self.vqvae(images)\n",
        "\n",
        "        # Path 2: Text-to-Image generation via Transformer\n",
        "        # Generate text embedding using DistilBERT\n",
        "        texts = [f\"{w}: {m}\" for w, m in zip(words, meanings)]\n",
        "\n",
        "        # Tokenize and encode the entire batch\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "        # Ensure inputs are on the same device as the model\n",
        "        device = next(self.text_encoder.parameters()).device # Get current device of the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        text_encoder_outputs = self.text_encoder(**inputs)\n",
        "        text_embedding = text_encoder_outputs.last_hidden_state[:, 0, :] # (batch_size, text_embed_dim)\n",
        "\n",
        "        # Pass text embedding to Transformer to get VQ code logits\n",
        "        vq_code_logits = self.transformer(text_embedding)\n",
        "\n",
        "        # Convert logits to discrete code indices\n",
        "        vq_code_indices = torch.argmax(vq_code_logits, dim=-1)\n",
        "\n",
        "        # Look up quantized embeddings from VQ-VAE's codebook\n",
        "        quantized_embeddings_flat = F.embedding(vq_code_indices, self.vqvae.vq.embedding.weight)\n",
        "\n",
        "        # Reshape to (batch_size, embedding_dim, latent_H, latent_W) for VQ-VAE decoder\n",
        "        quantized_embeddings_reshaped = quantized_embeddings_flat.view(\n",
        "            text_embedding.shape[0], self.latent_H, self.latent_W, self.vqvae.vq.embedding_dim\n",
        "        ).permute(0, 3, 1, 2) # (batch_size, embedding_dim, latent_H, latent_W)\n",
        "\n",
        "        # Pass reshaped latent embeddings through VQ-VAE's post_vq_conv and decoder\n",
        "        decoder_input = self.vqvae.post_vq_conv(quantized_embeddings_reshaped)\n",
        "        reconstructed_silhouette_from_text = self.vqvae.decoder(decoder_input)\n",
        "\n",
        "        return vq_loss, reconstructed_silhouette_from_text\n",
        "\n",
        "print(\"TextToSilhouetteModel class updated to handle batched inputs including images, and return VQ-VAE loss.\")\n",
        "\n",
        "# Re-instantiate the model to reflect the changes\n",
        "model = TextToSilhouetteModel()\n",
        "\n",
        "# Calculate total trainable parameters again, just to be sure.\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Re-instantiated TextToSilhouetteModel.\")\n",
        "print(f\"Total trainable parameters: {total_params / 1e6:.2f} million\")\n",
        "\n",
        "if total_params < 250e6:\n",
        "    print(\"Parameter count is still within the limit (under 250 million).\")\n",
        "else:\n",
        "    print(\"WARNING: Parameter count exceeds the limit of 250 million.\")\n",
        "\n",
        "# Now, set up the training loop.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 5 # Reduced for demonstration purposes\n",
        "\n",
        "print(f\"Starting training on {device} for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_epoch_loss = 0.0\n",
        "    total_vq_loss = 0.0\n",
        "    total_recon_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, words, meanings) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the TextToSilhouetteModel\n",
        "        vq_loss, reconstructed_silhouettes = model(images, list(words), list(meanings))\n",
        "\n",
        "        # Calculate reconstruction loss for the text-generated silhouette\n",
        "        recon_loss = reconstruction_criterion(reconstructed_silhouettes, images)\n",
        "\n",
        "        # Total loss combines VQ loss from images and reconstruction loss from text generation\n",
        "        loss = recon_loss + vq_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_vq_loss += vq_loss.item()\n",
        "        total_recon_loss += recon_loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0: # Log every 10 batches for larger datasets\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}]\\t\"\\\n",
        "                  f\"Total Loss: {loss.item():.4f}, VQ Loss: {vq_loss.item():.4f}, Recon Loss: {recon_loss.item():.4f}\")\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(dataloader)\n",
        "    avg_vq_loss = total_vq_loss / len(dataloader)\n",
        "    avg_recon_loss = total_recon_loss / len(dataloader)\n",
        "    print(f\"\\nEpoch {epoch+1} finished. Average Total Loss: {avg_epoch_loss:.4f}, Average VQ Loss: {avg_vq_loss:.4f}, Average Recon Loss: {avg_recon_loss:.4f}\\n\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextToSilhouetteModel class updated to handle batched inputs including images, and return VQ-VAE loss.\n",
            "Re-instantiated TextToSilhouetteModel.\n",
            "Total trainable parameters: 71.05 million\n",
            "Parameter count is still within the limit (under 250 million).\n",
            "Starting training on cpu for 5 epochs...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dummy_silhouette_images/silhouette_4.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2202905836.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mtotal_recon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-481920827.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_meaning_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_meaning_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dummy_silhouette_images/silhouette_4.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8c674a"
      },
      "source": [
        "**Reasoning**:\n",
        "The `FileNotFoundError` occurred because the `dummy_silhouette_images` directory, which was created in a previous cell, was subsequently removed in that same cell before the training loop in the current cell attempted to access it. To fix this, I need to recreate the dummy image directory, dummy images, `word_meaning_df`, and `dataloader` within this cell before starting the training loop, ensuring all necessary data is present when the model attempts to load images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9dc30e1"
      },
      "source": [
        "class TextToSilhouetteModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 # VQ-VAE parameters\n",
        "                 vqvae_in_channels=1, vqvae_num_hiddens=128, vqvae_num_residual_layers=2,\n",
        "                 vqvae_num_residual_hiddens=64, vqvae_num_embeddings=512,\n",
        "                 vqvae_embedding_dim=64, vqvae_commitment_cost=0.25,\n",
        "                 # Transformer parameters\n",
        "                 text_embed_dim=768, transformer_embed_dim=256, transformer_num_heads=8,\n",
        "                 transformer_num_layers=4, transformer_ff_dim=1024, transformer_dropout=0.1):\n",
        "        super(TextToSilhouetteModel, self).__init__()\n",
        "\n",
        "        # 1. Text Encoder (DistilBERT)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        # 2. VQ-VAE (Generative Backend part 1)\n",
        "        self.vqvae = VQVAE(in_channels=vqvae_in_channels,\n",
        "                           num_hiddens=vqvae_num_hiddens,\n",
        "                           num_residual_layers=vqvae_num_residual_layers,\n",
        "                           num_residual_hiddens=vqvae_num_residual_hiddens,\n",
        "                           num_embeddings=vqvae_num_embeddings,\n",
        "                           embedding_dim=vqvae_embedding_dim,\n",
        "                           commitment_cost=vqvae_commitment_cost)\n",
        "\n",
        "        # Calculate latent spatial dimensions for 128x128 image with Encoder's downsampling (4x)\n",
        "        # Encoder has two conv layers with stride 2 each, so total downsampling is 2*2 = 4\n",
        "        self.latent_H = 128 // (2*2) # 32\n",
        "        self.latent_W = 128 // (2*2) # 32\n",
        "        image_code_sequence_length = self.latent_H * self.latent_W # 32*32 = 1024\n",
        "\n",
        "        # 3. Transformer (Generative Backend part 2)\n",
        "        self.transformer = Transformer(text_embed_dim=text_embed_dim,\n",
        "                                       num_vq_embeddings=vqvae_num_embeddings, # Output logits for VQ-VAE codebook entries\n",
        "                                       image_code_sequence_length=image_code_sequence_length,\n",
        "                                       embed_dim=transformer_embed_dim,\n",
        "                                       num_heads=transformer_num_heads,\n",
        "                                       num_layers=transformer_num_layers,\n",
        "                                       ff_dim=transformer_ff_dim,\n",
        "                                       dropout=transformer_dropout)\n",
        "\n",
        "    def forward(self, images, words, meanings):\n",
        "        # Path 1: VQ-VAE loss calculation from input images\n",
        "        # This trains the VQ-VAE to correctly encode/decode images and learn its codebook\n",
        "        vq_loss, vqvae_reconstructions_from_images, _ = self.vqvae(images)\n",
        "\n",
        "        # Path 2: Text-to-Image generation via Transformer\n",
        "        # Generate text embedding using DistilBERT\n",
        "        texts = [f\"{w}: {m}\" for w, m in zip(words, meanings)]\n",
        "\n",
        "        # Tokenize and encode the entire batch\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "        # Ensure inputs are on the same device as the model\n",
        "        device = next(self.text_encoder.parameters()).device # Get current device of the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        text_encoder_outputs = self.text_encoder(**inputs)\n",
        "        text_embedding = text_encoder_outputs.last_hidden_state[:, 0, :] # (batch_size, text_embed_dim)\n",
        "\n",
        "        # Pass text embedding to Transformer to get VQ code logits\n",
        "        vq_code_logits = self.transformer(text_embedding)\n",
        "\n",
        "        # Convert logits to discrete code indices\n",
        "        vq_code_indices = torch.argmax(vq_code_logits, dim=-1)\n",
        "\n",
        "        # Look up quantized embeddings from VQ-VAE's codebook\n",
        "        quantized_embeddings_flat = F.embedding(vq_code_indices, self.vqvae.vq.embedding.weight)\n",
        "\n",
        "        # Reshape to (batch_size, embedding_dim, latent_H, latent_W) for VQ-VAE decoder\n",
        "        quantized_embeddings_reshaped = quantized_embeddings_flat.view(\n",
        "            text_embedding.shape[0], self.latent_H, self.latent_W, self.vqvae.vq.embedding_dim\n",
        "        ).permute(0, 3, 1, 2) # (batch_size, embedding_dim, latent_H, latent_W)\n",
        "\n",
        "        # Pass reshaped latent embeddings through VQ-VAE's post_vq_conv and decoder\n",
        "        decoder_input = self.vqvae.post_vq_conv(quantized_embeddings_reshaped)\n",
        "        reconstructed_silhouette_from_text = self.vqvae.decoder(decoder_input)\n",
        "\n",
        "        return vq_loss, reconstructed_silhouette_from_text\n",
        "\n",
        "print(\"TextToSilhouetteModel class updated to handle batched inputs including images, and return VQ-VAE loss.\")\n",
        "\n",
        "# Re-instantiate the model to reflect the changes\n",
        "model = TextToSilhouetteModel()\n",
        "\n",
        "# Calculate total trainable parameters again, just to be sure.\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Re-instantiated TextToSilhouetteModel.\")\n",
        "print(f\"Total trainable parameters: {total_params / 1e6:.2f} million\")\n",
        "\n",
        "if total_params < 250e6:\n",
        "    print(\"Parameter count is still within the limit (under 250 million).\")\n",
        "else:\n",
        "    print(\"WARNING: Parameter count exceeds the limit of 250 million.\")\n",
        "\n",
        "# Re-create dummy data for training to resolve FileNotFoundError\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil # Import shutil for cleanup\n",
        "\n",
        "# 1. Create a dummy image directory and dummy images\n",
        "dummy_image_dir = 'dummy_silhouette_images'\n",
        "# Remove directory if it exists from previous runs to ensure clean state\n",
        "if os.path.exists(dummy_image_dir):\n",
        "    shutil.rmtree(dummy_image_dir)\n",
        "os.makedirs(dummy_image_dir, exist_ok=True)\n",
        "\n",
        "# Create 5 dummy grayscale images (128x128)\n",
        "num_dummy_images = 5\n",
        "image_filenames = []\n",
        "for i in range(num_dummy_images):\n",
        "    img_array = np.random.randint(0, 256, (128, 128), dtype=np.uint8)\n",
        "    img = Image.fromarray(img_array, mode='L') # 'L' for grayscale\n",
        "    filename = f'silhouette_{i+1}.png'\n",
        "    img.save(os.path.join(dummy_image_dir, filename))\n",
        "    image_filenames.append(filename)\n",
        "\n",
        "# 2. Create a dummy word_meaning_df\n",
        "dummy_data = {\n",
        "    'image_filename': image_filenames,\n",
        "    'word': ['cat', 'dog', 'house', 'tree', 'car'],\n",
        "    'meaning': [\n",
        "        'a small domesticated carnivorous mammal with soft fur, a short snout, and retractile claws.',\n",
        "        'a domesticated carnivorous mammal that typically has a long snout, an acute sense of smell, non-retractable claws, and a barking, howling, or whining voice.',\n",
        "        'a building for human habitation, especially one that is lived in by a family or small group of people.',\n",
        "        'a woody perennial plant, typically having a single stem or trunk growing to a considerable height and bearing lateral branches at some distance from the ground.',\n",
        "        'a road vehicle, typically with four wheels, powered by an internal combustion engine or electric motor and able to carry a small number of people.'\n",
        "    ]\n",
        "}\n",
        "word_meaning_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "# 3. Define transformations (copied from previous cell for self-containment)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
        "    transforms.Grayscale(num_output_channels=1), # Ensure grayscale\n",
        "    transforms.ToTensor(),          # Convert to PyTorch tensor (scales to [0, 1])\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Instantiate dataset and dataloader (copied from previous cell for self-containment)\n",
        "dataset = CustomSilhouetteDataset(image_dir=dummy_image_dir, word_meaning_df=word_meaning_df, transform=transform)\n",
        "batch_size = 2 # Small batch size for demonstration\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Dummy data and dataloader re-prepared for training loop.\")\n",
        "\n",
        "# Now, set up the training loop.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 5 # Reduced for demonstration purposes\n",
        "\n",
        "print(f\"Starting training on {device} for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_epoch_loss = 0.0\n",
        "    total_vq_loss = 0.0\n",
        "    total_recon_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, words, meanings) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the TextToSilhouetteModel\n",
        "        # Pass words and meanings as lists as expected by the modified forward method\n",
        "        vq_loss, reconstructed_silhouettes = model(images, list(words), list(meanings))\n",
        "\n",
        "        # Calculate reconstruction loss for the text-generated silhouette\n",
        "        recon_loss = reconstruction_criterion(reconstructed_silhouettes, images)\n",
        "\n",
        "        # Total loss combines VQ loss from images and reconstruction loss from text generation\n",
        "        loss = recon_loss + vq_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_epoch_loss += loss.item()\n",
        "        total_vq_loss += vq_loss.item()\n",
        "        total_recon_loss += recon_loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0: # Log every 10 batches for larger datasets\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}]\\t\"\\\n",
        "                  f\"Total Loss: {loss.item():.4f}, VQ Loss: {vq_loss.item():.4f}, Recon Loss: {recon_loss.item():.4f}\")\n",
        "\n",
        "    avg_epoch_loss = total_epoch_loss / len(dataloader)\n",
        "    avg_vq_loss = total_vq_loss / len(dataloader)\n",
        "    avg_recon_loss = total_recon_loss / len(dataloader)\n",
        "    print(f\"\\nEpoch {epoch+1} finished. Average Total Loss: {avg_epoch_loss:.4f}, Average VQ Loss: {avg_vq_loss:.4f}, Average Recon Loss: {avg_recon_loss:.4f}\\n\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Optional: Clean up dummy directory after training\n",
        "shutil.rmtree(dummy_image_dir)\n",
        "print(f\"Dummy image directory '{dummy_image_dir}' removed after training.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9bc9be2"
      },
      "source": [
        "**Reasoning**:\n",
        "The training loop executed successfully in the previous step. Now I will add a new text cell to formally introduce the next subtask: 'Evaluate Model Performance'. This aligns with the logical progression of model development after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13372903"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained text-to-silhouette model using appropriate metrics. This includes generating silhouettes from new text descriptions and visually inspecting their quality, as well as considering quantitative metrics if applicable."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052023c7"
      },
      "source": [
        "# Task\n",
        "Generate and display silhouettes for a few example word-meaning pairs using the trained `TextToSilhouetteModel` and visually inspect their quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75299597"
      },
      "source": [
        "## Generate Example Silhouettes\n",
        "\n",
        "### Subtask:\n",
        "Use the trained `TextToSilhouetteModel` to generate silhouettes for a few example word-meaning pairs. This will involve putting the model in evaluation mode and passing text inputs to it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e48e145"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to set the model to evaluation mode, prepare example text inputs, and call the model's forward method to generate silhouettes. Since the current `forward` method of `TextToSilhouetteModel` expects an `images` tensor (which is used for VQ-VAE training), I'll pass a dummy tensor for `images` during inference and extract only the `reconstructed_silhouettes` output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1550b527"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Set the model to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model set to evaluation mode.\")\n",
        "\n",
        "# 2. Create example words and meanings\n",
        "example_words = [\"bird\", \"cup\", \"chair\"]\n",
        "example_meanings = [\n",
        "    \"a warm-blooded egg-laying vertebrate animal distinguished by the possession of feathers, wings, a beak, and typically by being able to fly.\",\n",
        "    \"a drinking vessel, typically made of porcelain or earthenware, with a handle.\",\n",
        "    \"a separate seat for one person, typically with a back and four legs.\"\n",
        "]\n",
        "\n",
        "# Ensure the model is on the correct device\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "# 3. Pass dummy images and example text to the model's forward method\n",
        "# The VQ-VAE part of the forward pass still requires an `images` input.\n",
        "# For inference, we can pass a dummy tensor and ignore its VQ-VAE loss.\n",
        "# Shape should match expected input: (batch_size, channels, H, W)\n",
        "dummy_images = torch.randn(len(example_words), 1, 128, 128).to(device)\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculations for inference\n",
        "    vq_loss, generated_silhouettes = model(dummy_images, example_words, example_meanings)\n",
        "\n",
        "# 4. Move generated silhouettes to CPU and detach\n",
        "generated_silhouettes_cpu = generated_silhouettes.detach().cpu()\n",
        "\n",
        "print(f\"Generated silhouettes shape: {generated_silhouettes_cpu.shape}\")\n",
        "\n",
        "# Optional: Visualize the generated silhouettes\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, silhouette in enumerate(generated_silhouettes_cpu):\n",
        "    plt.subplot(1, len(example_words), i + 1)\n",
        "    # Un-normalize the image from [-1, 1] to [0, 1] for display\n",
        "    img_display = (silhouette.squeeze().numpy() + 1) / 2\n",
        "    plt.imshow(img_display, cmap='gray')\n",
        "    plt.title(f\"{example_words[i]}\\n({example_meanings[i][:20]}...)\")\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Generated Silhouettes from Text Descriptions\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}